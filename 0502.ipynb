{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSBjMd1ZErvUZlMUM6f51t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengpeilee/Fit-Meets/blob/master/0502.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello World\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWn1LGL15RQ3",
        "outputId": "6f0f36fc-8504-4d87-9991-b52648c173f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# String\n",
        "\n",
        "word = \"Hello World\"\n",
        "\n",
        "\n",
        "# Number\n",
        "\n",
        "num = 10\n",
        "\n",
        "\n",
        "# Boolean\n",
        "\n",
        "is_true = True\n",
        "is_false = False\n",
        "\n",
        "# List\n",
        "\n",
        "scores = [90, 80, 100, 65]\n",
        "sports = [\"baseball\", \"badminton\"]\n",
        "\n",
        "\n",
        "# Dictionary: Key / Value\n",
        "\n",
        "my_grades = {\n",
        "    'Math': 90,\n",
        "    'English': 80,\n",
        "    'Science': 100,\n",
        "    'History': 65\n",
        "}"
      ],
      "metadata": {
        "id": "db3LpzOi8AWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greet = \"How're you doing\"\n",
        "res = \"Great, how're you?\"\n",
        "\n",
        "comb = greet + res\n",
        "\n",
        "print(comb)\n",
        "\n",
        "# Variable\n",
        "# 1. 分類好我要的句子\n",
        "# 2. 方便更改"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x-eRN-L8bqi",
        "outputId": "35254593-4563-4fe6-f611-c088f7ff386c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How're you doingGreat, how're you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_1 = 3\n",
        "num_2 = 10\n",
        "\n",
        "num_3 = num_1 + num_2\n",
        "num_4 = num_2 / num_1 # 10 除以 5\n",
        "num_5 = num_2 % num_1 # 10 餘數\n",
        "num_6 = num_2 - num_1 # 10 減去 5\n",
        "num_7 = num_2 * num_1 # 10 乘以 5\n",
        "\n",
        "\n",
        "print(num_3)\n",
        "print(num_4)\n",
        "print(num_5)\n",
        "print(num_6)\n",
        "print(num_7)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdfXyp8P8zdy",
        "outputId": "1c6bf870-e1ae-4afc-8335-6b421f787732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "3.3333333333333335\n",
            "1\n",
            "7\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [90, 80, 100, 65]\n",
        "sports = [\"baseball\", \"badminton\"]\n",
        "\n",
        "# 想像有一個櫃子，來存放所有的資料\n",
        "\n",
        "# list index: start with 0\n",
        "\n",
        "# print(scores[0])\n",
        "# print(scores[1])\n",
        "# print(scores[2])\n",
        "# print(scores[3])\n",
        "\n",
        "\n",
        "print(sports[0])\n",
        "print(sports[1])\n",
        "# print(sports[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5y-s3j39wZM",
        "outputId": "3e29f2c0-2ce8-4d52-f588-d32f47b48749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseball\n",
            "badminton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_letter = [\"share\", \"account\", \"passcode\"]\n",
        "\n",
        "print(list_letter[0])\n",
        "print(list_letter[0] + list_letter[1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxpibCk7-nWl",
        "outputId": "187e01b3-76c8-4964-b9b9-a3768a272573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "share\n",
            "shareaccount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animal = \"Dog is an Animal\"\n",
        "\n",
        "# Function: 已經建立好的功能\n",
        "\n",
        "# len: 計算字串長度\n",
        "\n",
        "# len(your_var_name)\n",
        "\n",
        "# lower: 轉換字串為小寫 str.lower()\n",
        "\n",
        "\n",
        "# upper: 轉換字串為大小 str.upper()\n",
        "\n",
        "new_animal_lower = animal.lower()\n",
        "\n",
        "new_animal_upper = animal.upper()\n",
        "\n",
        "print(new_animal_lower)\n",
        "print(new_animal_upper)\n",
        "print(len(animal))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQQZW4Xq_bEY",
        "outputId": "70c07e1a-e565-41db-bbe7-b7b3f6a6feff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog is an animal\n",
            "DOG IS AN ANIMAL\n",
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 課堂練習 1:\n",
        "\n",
        "sent = \"Today is a good day!\"\n",
        "\n",
        "# 1: print 出句子長度\n",
        "\n",
        "sent_len = len(sent)\n",
        "print(sent_len)\n",
        "\n",
        "# 2: 把句子轉成大寫\n",
        "\n",
        "sent_upper = sent.upper()\n",
        "print(sent_upper)\n",
        "\n",
        "# 3: 把句子轉成小寫\n",
        "\n",
        "sent_lower = sent.lower()\n",
        "print(sent_lower)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjRxSKGKAse0",
        "outputId": "a5d3849c-94b7-442d-c06e-5f140e518460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "TODAY IS A GOOD DAY!\n",
            "today is a good day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_1 = \"San Francisco\"\n",
        "sting_2 = \"Silicon Valley\"\n",
        "string_3 = \"Taipei\"\n",
        "string_4 = \"101\"\n",
        "\n",
        "\n",
        "print(f\"Let's go to {string_1}. It's a beautiful city\")\n",
        "print(f\"Let's go to {string_3} {string_4}. It's a beautiful building\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOGzWDCfDXDz",
        "outputId": "a3ae5ae3-a20f-46d6-ac4c-7b4c730a8081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's go to San Francisco. It's a beautiful city\n",
            "Let's go to Taipei 101. It's a beautiful city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 課堂練習 2\n",
        "\n",
        "\n",
        "# 請寫出2個 variables, 並利用 print 出\n",
        "\n",
        "a = \"Test\"\n",
        "\n",
        "b = \"English\"\n",
        "\n",
        "print(f\"The Subject of {a} is {b}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1hw4Tj9EKak",
        "outputId": "ba68fa01-34ed-4953-dfd9-331ee78810ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Subject of Test is English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional 條件式邏輯\n",
        "\n",
        "\n",
        "box = \"apple\"\n",
        "\n",
        "\n",
        "\n",
        "if box == \"apple\":\n",
        "  print(\"yes\")\n",
        "else:\n",
        "  print(\"no\")\n",
        "\n",
        "\n",
        "# 1.當今天我們要判斷 var 是否等於某個值時，要用 ==\n",
        "\n",
        "# = : assign\n",
        "# == : equal\n",
        "\n",
        "\n",
        "# 2.indent\n",
        "\n",
        "\n",
        "# 3. conditional 結尾 ：\n",
        "\n",
        "# 4. if elif else - else 後面不需要有條件\n",
        "\n",
        "\n",
        "# 5. 如果有更多條件？\n",
        "\n",
        "\n",
        "# 情境2: 去超市買了apple，如果沒有頻果我就買香蕉。如果都沒有，就顯示找不到\n",
        "\n",
        "\n",
        "b = \"pineapple\"\n",
        "\n",
        "\n",
        "if b == \"apple\":\n",
        "  print(\"yes, I find an apple\")\n",
        "elif b == \"banana\": # else if\n",
        "  print(\"yes\")\n",
        "  print(\"I find a banana\")\n",
        "elif b == \"pineapple\":\n",
        "  print(\"yes\")\n",
        "  print(\"I find a pineapple\")\n",
        "elif b == \"orange\":\n",
        "  print(\"yes\")\n",
        "  print(\"I find an orange\")\n",
        "elif b == \"grape\":\n",
        "  print(\"yes\")\n",
        "  print(\"I find a grape\")\n",
        "else:\n",
        "  print(\"no\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV4KRcalIRoa",
        "outputId": "959ae496-9d34-436f-e012-f3e9be02a4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function\n",
        "\n",
        "# upper() lower() len() Python 內建的function\n",
        "\n",
        "\n",
        "\n",
        "def burgerMarker(material_1, material_2, material_3, material_4):\n",
        "  burger = material_1 + \" \" + material_2 + \" \" + material_3 + \" \" + material_4\n",
        "  return burger\n",
        "\n",
        "\n",
        "a = \"meat\"\n",
        "b = \"cheese\"\n",
        "c = \"bread\"\n",
        "d = \"salad\"\n",
        "\n",
        "\n",
        "res = burgerMarker(a, b, c, d)\n",
        "\n",
        "print(res)\n",
        "\n",
        "\n",
        "def printHelloWorld():\n",
        "  print(\"Hello World\")\n",
        "\n",
        "\n",
        "printHelloWorld()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQIUZv4mKr1A",
        "outputId": "322cadaa-e964-4e12-82b4-34a9432ea40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meat cheese bread salad\n",
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function 的概念是把程式碼封裝起來重複使用\n",
        "\n",
        "#字元偵測\n",
        "\n",
        "def wordDetect(s):\n",
        "  if s == s.lower():\n",
        "    print(\"lower\")\n",
        "  elif s == s.upper():\n",
        "    print(\"upper\")\n",
        "  else:\n",
        "    print(\"mixed\")\n",
        "\n",
        "\n",
        "sting_text = \"HELLO\"\n",
        "\n",
        "wordDetect(sting_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S58HcF99Ml2p",
        "outputId": "1d98ab72-bec2-49ab-a735-16fbfeace977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mixed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習 3: 模仿 wrodDetect function，並試著寫一些範例test 測試\n",
        "# 試著改寫 wordDetect\n",
        "\n",
        "def wordDetect(s):\n",
        "\n",
        "  if s == s.lower():\n",
        "    print(\"lower\")\n",
        "  elif s == s.upper():\n",
        "    print(\"upper\")\n",
        "  else:\n",
        "    print(\"mixed\")\n",
        "\n",
        "\n",
        "sent = \"Today is a good day\"\n",
        "\n",
        "wordDetect(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OulIbZv6Nqdi",
        "outputId": "bbf4095f-1524-4325-ea0b-67a2c2b4be0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "too long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List\n",
        "\n",
        "\n",
        "zoo_animals = [\"Dog\", \"Cat\", \"Bird\",\"Tiger\", \"Elephant\"];\n",
        "\n",
        "\n",
        "print(zoo_animals[0])\n",
        "print(zoo_animals[1])\n",
        "print(zoo_animals[2])\n",
        "print(zoo_animals[3])\n",
        "print(zoo_animals[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78DfHYD_CY0R",
        "outputId": "0dab8cb6-07dd-4cff-b683-4bb0bc4a99b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dog\n",
            "Cat\n",
            "Bird\n",
            "Tiger\n",
            "Elephant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List Slicing\n",
        "\n",
        "zoo_animals = [\"Dog\", \"Cat\", \"Bird\",\"Tiger\", \"Elephant\"];\n",
        "\n",
        "# print(zoo_animals[0:2]) # 0 開始 2 之前\n",
        "# print(zoo_animals[0:3])\n",
        "# print(zoo_animals[1:3])\n",
        "print(zoo_animals[2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApHCBf19Cqeo",
        "outputId": "f52e20ac-3b47-4af8-8ccd-e0fe705fc4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bird', 'Tiger', 'Elephant']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = \"catdogBird\"\n",
        "\n",
        "cat = animals[0:3]\n",
        "dog = animals[3:6]\n",
        "bird = animals[6:]\n",
        "\n",
        "print(cat)\n",
        "print(dog)\n",
        "print(bird)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKyGqFm5DSSw",
        "outputId": "cbdd69fd-2887-413f-8de7-a75ef677a1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "dog\n",
            "Bird\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python 裡已經寫好的function，可以透過 import 引用更多 library\n",
        "\n",
        "import math\n",
        "\n",
        "print(math.sqrt(25))\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "print(nltk.download())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jKSTUiiDqmk",
        "outputId": "13edba09-6256-43f0-d493-ff2c8626bd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop\n",
        "\n",
        "\n",
        "a = 5\n",
        "b = 6\n",
        "c = 8\n",
        "d = 9\n",
        "\n",
        "# 讓所有的數字都乘以 2\n",
        "\n",
        "# print(a * 2)\n",
        "# print(b * 2)\n",
        "# print(c * 2)\n",
        "# print(d * 2)\n",
        "\n",
        "num_list = [5, 7, 8, 9, 10, 12]\n",
        "\n",
        "\n",
        "# for x in num_list:\n",
        "\n",
        "# 1st : num = 5\n",
        "# 2nd : num = 7\n",
        "# 3rd : num = 8\n",
        "# 4th : num = 9\n",
        "# 5th : num = 10\n",
        "# 6th : num = 12\n",
        "\n",
        "for num in num_list:\n",
        "  print(num * 2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7BIA-8dEWrq",
        "outputId": "83a986e6-93ae-4fe5-b048-77e92cd33929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "14\n",
            "16\n",
            "18\n",
            "20\n",
            "24\n",
            "[5, 7, 8, 9, 10, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zoo_animals = [\"Dog\", \"Cat\", \"Bird\",\"Tiger\", \"Elephant\"];\n",
        "\n",
        "# print(zoo_animals[0])\n",
        "# print(zoo_animals[1])\n",
        "# print(zoo_animals[2])\n",
        "# print(zoo_animals[3])\n",
        "# print(zoo_animals[4])\n",
        "\n",
        "\n",
        "# 1st: animal = \"Dog\"\n",
        "# 2nd: animal = \"Cat\"\n",
        "# 3rd: animal = \"Bird\"\n",
        "# 4th: animal = \"Tiger\"\n",
        "# 5th: animal = \"Elephant\"\n",
        "\n",
        "for animal in zoo_animals:\n",
        "  print(animal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR-Yvn8vGCcC",
        "outputId": "d9cdfe9b-12f1-4e46-f8f2-444c8a3f6ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dog\n",
            "Cat\n",
            "Bird\n",
            "Tiger\n",
            "Elephant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zoo_animals = [\"Dog\", \"Cat\", \"Bird\",\"Tiger\", \"Elephant\"];\n",
        "\n",
        "\n",
        "# 練習1： 使用 for 迴圈 print 出裡面的動物並且加上複數\n",
        "\n",
        "for animal in zoo_animals:\n",
        "  print(animal + \"s\")\n",
        "\n",
        "\n",
        "# 練習2: 使用 for 迴圈 print 長度> 5 的動物\n",
        "\n",
        "\n",
        "for animal in zoo_animals:\n",
        "  if len(animal) > 5:\n",
        "    print(animal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puqEyljwGiyB",
        "outputId": "3174af3c-5c1f-4c3b-b0f3-c419df033add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dogs\n",
            "Cats\n",
            "Birds\n",
            "Tigers\n",
            "Elephants\n",
            "Elephant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary: Key / Value\n",
        "\n",
        "my_grades = {\n",
        "    'Math': 90,\n",
        "    'English': 80,\n",
        "    'Science': 100,\n",
        "    'History': 65,\n",
        "    'Geography': 'NO SHOW',\n",
        "    'Chinese': {\n",
        "        'Writing': 90,\n",
        "        'Reading': 80,\n",
        "        'Speaking': 100\n",
        "    }\n",
        "}\n",
        "\n",
        "print(my_grades['Math'])\n",
        "print(my_grades['English'])\n",
        "print(my_grades['Science'])\n",
        "print(my_grades['History'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKoljUvdKvSv",
        "outputId": "7f5d58bf-1d81-4646-f68b-092a17726459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "80\n",
            "100\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 參考程式碼\n",
        "# https://www.nltk.org/book/\n",
        "# https://github.com/youngmihuang/NLTK"
      ],
      "metadata": {
        "id": "CgBUF93CLmur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "nltk.download(\"all-corpora\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcBfgO5MNbcM",
        "outputId": "4a01f867-622d-4294-91a3-71bb7e5e1025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading collection 'all-corpora'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all-corpora\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.book import *\n",
        "\n",
        "# text1: Moby Dick by Herman Melville 1851\n",
        "# text2: Sense and Sensibility by Jane Austen 1811\n",
        "# text3: The Book of Genesis\n",
        "# text4: Inaugural Address Corpus\n",
        "# text5: Chat Corpus\n",
        "# text6: Monty Python and the Holy Grail\n",
        "# text7: Wall Street Journal\n",
        "# text8: Personals Corpus\n",
        "# text9: The Man Who Was Thursday by G . K . Chesterton 1908"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mUA4W7HPRvT",
        "outputId": "4f042c4f-7fd5-4bd9-f23f-d0585fb0981c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 搜尋字詞功能\n",
        "\n",
        "import nltk\n",
        "from nltk.book import *\n",
        "\n",
        "\n",
        "text3.count(\"people\")\n",
        "\n",
        "# text3.concordance(\"lived\", lines = 50)\n",
        "text3.concordance(\"people\", lines = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqJ94Hy3PusW",
        "outputId": "3eed09cb-2a23-490f-f7db-a5f6ef593fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 30 of 35 matches:\n",
            " . And the LORD said , Behold , the people is one , and they have all one lang\n",
            "oods , and the women also , and the people . And the king of Sodom went out to\n",
            "that soul shall be cut off from his people ; he hath broken my covenant . And \n",
            "l be a mother of nations ; kings of people shall be of her . Then Abraham fell\n",
            "ound , both old and young , all the people from every quart And they called un\n",
            "stood up , and bowed himself to the people of the land , even to the children \n",
            "; in the presence of the sons of my people give I it th bury thy dead . And Ab\n",
            "raham bowed down himself before the people of the land . And he spake unto Eph\n",
            " unto Ephron in the audience of the people of the land , saying , But if thou \n",
            " of years ; and was gathered to his people . And his sons Isaac and Ishmael bu\n",
            "nd died ; and was gathered unto his people . And they dwelt from Havilah unto \n",
            "are in thy womb , and two manner of people shall be separated from thy bowels \n",
            "rated from thy bowels ; and the one people shall be stronger than the other pe\n",
            "le shall be stronger than the other people ; and the elder shall serve the you\n",
            "thou hast done unto us ? one of the people might lightly have lien with thy wi\n",
            " us . And Abimelech charged all his people , saying , He that toucheth this ma\n",
            "rth , and plenty of corn and wi Let people serve thee , and nations bow down t\n",
            " that thou mayest be a multitude of people ; And give thee the blessing of Abr\n",
            "ney , and came into the land of the people of the east . And he looked , and b\n",
            "and distressed : and he divided the people that was with him , and the flocks \n",
            "l with you , and we will become one people . But if ye will not hearken unto u\n",
            "us for to dwell with us , to be one people , if every male among us be circumc\n",
            ", that is , Bethel , he and all the people that were with him . And he built t\n",
            "nd died , and was gathered unto his people , being old and full of da and his \n",
            "ccording unto thy word shall all my people be rul only in the throne will I be\n",
            "he land of Egypt was famished , the people cried to Pharaoh for bre and Pharao\n",
            " and he it was that sold to all the people of the la and Joseph ' s brethren c\n",
            "became Pharaoh ' s . And as for the people , he removed them to cities from on\n",
            "r lands . Then Joseph said unto the people , Behold , I have bought you this d\n",
            " I will make of thee a multitude of people ; and will give this land to thy se\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text1: Moby Dick by Herman Melville 1851\n",
        "# text2: Sense and Sensibility by Jane Austen 1811\n",
        "# text3: The Book of Genesis\n",
        "# text4: Inaugural Address Corpus\n",
        "# text5: Chat Corpus\n",
        "# text6: Monty Python and the Holy Grail\n",
        "# text7: Wall Street Journal\n",
        "# text8: Personals Corpus\n",
        "# text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
        "\n",
        "'''\n",
        "根據該詞的上下文，找到類似結構，就認定他們為近似字。假設我們現在要在 text1 裡找\n",
        "monstrous 字詞，而 monstrous 會出現在 the ___ pictures 以及 a ___ size\n",
        "這樣的結構當中，透過這個方法去比對，以下字詞( true、 contemptible 、 christian )\n",
        "會在 text1 文本出現在一樣的結構中，就認定他們為近似字。\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "from nltk.book import *\n",
        "\n",
        "# 近似字\n",
        "\n",
        "text1.similar(\"monstrous\")\n",
        "\n",
        "print (\"----------我是分隔線-------\")\n",
        "\n",
        "text2.similar(\"monstrous\")\n",
        "\n",
        "\n",
        "# common_contexts\n",
        "\n",
        "print (\"----------我是分隔線-------\")\n",
        "\n",
        "context = [\"monstrous\", \"good\", \"great\"]\n",
        "\n",
        "text2.common_contexts(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_q3zKeZQleq",
        "outputId": "e6419d29-0f72-4cc7-e756-fae4e2b7a524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true contemptible christian abundant few part mean careful puzzled\n",
            "mystifying passing curious loving wise doleful gamesome singular\n",
            "delightfully perilous fearless\n",
            "----------我是分隔線-------\n",
            "very so exceedingly heartily a as good great extremely remarkably\n",
            "sweet vast amazingly\n",
            "----------我是分隔線-------\n",
            "a_deal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習 count, concordance, similar, common_contexts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PzAmufR9R8KN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}